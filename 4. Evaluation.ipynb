{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This section provides a detailed comparative analysis of the models based on their performance metrics. The primary goal is to develop and deploy a robust restaurant recommender system to help customers find the best dining experience.\n",
    "\n",
    "\n",
    "In this project, we will concentrate on three specific types of recommendation models:\n",
    "\n",
    "- Content-Based Recommender Systems\n",
    "\n",
    "\n",
    "- Collaborative Filtering Systems\n",
    "\n",
    "\n",
    "- Deep Neural Networks\n",
    "\n",
    "\n",
    "Within each category, we will evaluate and compare different models to determine which performs the best. For validation and comparison, we will use the RMSE (root mean squared error) metric to measure how closely the predictions align with the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENT BASED MODELS\n",
    "\n",
    "\n",
    "#### 1) Baseline model - SVD\n",
    "\n",
    "**Vanilla model**\n",
    "\n",
    "The findings for the vanilla SVD include:\n",
    "\n",
    "- **Model Performance Consistency**: The RMSE values across the five cross-validation folds are very close, ranging from 1.213 to 1.220. This indicates that the SVD model is performing consistently, with minimal variation in prediction accuracy across different subsets of the data.\n",
    "\n",
    "\n",
    "\n",
    "- **Average Model Accuracy**: The mean RMSE of 1.217 suggests that, on average, the model's predictions are approximately 1.217 units away from the true values. While this provides a baseline for model accuracy, further tuning or alternative models might be explored to reduce this error.\n",
    "\n",
    "\n",
    "\n",
    "- **Error Analysis with MAE**: The Mean Absolute Error (MAE) values, ranging from 0.946 to 0.950, confirm that the average absolute error is less than 1 unit. MAE complements RMSE by offering a straightforward interpretation of prediction accuracy without heavily penalizing larger errors.\n",
    "\n",
    "\n",
    "\n",
    "- **Training Time Variability**: The fit times for the five folds vary significantly, from around 8.37 to 16.70 seconds. This variability could be due to computational resource differences or data partitioning, but it generally suggests that the model is not computationally expensive to train.\n",
    "\n",
    "\n",
    "\n",
    "- **Efficient Prediction**: The test times are relatively low, with the longest time being just over 2 seconds. This indicates that once the model is trained, it can make predictions quickly, making it suitable for real-time or near-real-time recommendation tasks.\n",
    "\n",
    "\n",
    "The model is however displaying a few shortcomings including:\n",
    "\n",
    "- **Moderate RMSE**: The mean RMSE of 1.217 indicates that the predictions are moderately accurate, but there may still be room for improvement. Exploring different models or fine-tuning the SVD parameters could potentially reduce this error.\n",
    "\n",
    "\n",
    "- **Fit Time Variability**: The significant variation in training times (ranging from 8.37 to 16.70 seconds) could be a concern in scenarios where consistent training time is critical. This variability might indicate that the model's performance could be impacted by differences in data partitioning or computational resources.\n",
    "\n",
    "\n",
    "- **Potential Overfitting**: The cross-validation results are consistent, but without additional checks (like testing on a completely separate validation set), there's a slight risk of overfitting, especially if the model is overly tailored to the specific folds used in cross-validation.\n",
    "\n",
    "\n",
    "To adress these shortcomings,we carry out  hyperparameter tuning for the SVD model using GridSearchCV. It defines a parameter grid with different values for latent factors and regularization terms. GridSearchCV is then used to evaluate these parameters across the dataset to find the best combination. The model is fitted with the dataset to determine the optimal hyperparameters that yield the best performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuned model**\n",
    "\n",
    "The observations for the tuned model include:\n",
    "\n",
    "- Minimal Improvement: The best RMSE after hyperparameter tuning (1.2171) is very close to the initial mean RMSE of 1.2170, indicating that the tuning provided only a slight improvement. This suggests that the model's performance might be close to its maximum potential for this specific dataset and approach.\n",
    "\n",
    "\n",
    "- Higher Complexity: The optimal number of factors (n_factors = 100) is on the higher end of the tested range, which could imply that the model benefits from increased complexity, capturing more intricate patterns in the data.\n",
    "\n",
    "\n",
    "- Low Regularization: The best regularization term (reg_all = 0.02) is relatively low, indicating that the model performs better with less penalty on the size of the coefficients. This may suggest that the model is less prone to overfitting on this dataset.\n",
    "\n",
    "\n",
    "In conclusion,the hyperparameter tuning has fine-tuned the SVD model slightly, but the improvement in performance is minimal. It may be beneficial to explore other models or more advanced feature engineering to achieve more significant gains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Deep Neural Networks (DNN)\n",
    "\n",
    "We will employ a Keras deep neural network to develop a recommendation system, aiming to enhance our RMSE scores through the use of neural network techniques.\n",
    "\n",
    "**Vanilla model**\n",
    "\n",
    "The Keras code defines a deep neural network for a recommendation system with a three-layer architecture: the first dense layer has 30 nodes with ReLU activation, the second dense layer has 15 nodes with ReLU activation, and the output layer has one node with a sigmoid activation to produce ratings between 0 and 1. A Lambda layer scales these predictions to a range of 1 to 5. The model is compiled using SGD optimizer, MSE loss function, and RMSE as a metric. It is trained on X_train and y_train, with validation on X_test and y_test, for 10 epochs with a batch size of 256.\n",
    "\n",
    "Here’s a summary of the model training results over 10 epochs:\n",
    "\n",
    "- Epoch 1: Initial RMSE is 1.5015 with a validation RMSE of 1.5007. The training loss is 2.2653.\n",
    "\n",
    "- Epoch 2-4: RMSE improves slightly to 1.4983 in epoch 2, and remains stable around 1.4998 to 1.5011 in subsequent epochs. The validation RMSE also stabilizes around 1.5002 to 1.5005.\n",
    "\n",
    "- Epoch 5-7: Further improvement is observed with RMSE decreasing to 1.4994 by epoch 5 and to 1.4993 by epoch 7. The validation RMSE decreases to 1.4980.\n",
    "\n",
    "- Epoch 8-10: Significant improvement is seen with RMSE dropping to 1.4950 by epoch 8, and further decreasing to 1.4780 by epoch 9. By epoch 10, RMSE reaches 1.4409, with the validation RMSE at 1.4512.\n",
    "\n",
    "I conclusion, the model shows a gradual improvement in RMSE and validation RMSE throughout the epochs, with the most significant gains occurring in the later epochs.\n",
    "\n",
    "\n",
    "**Tuned model 1**\n",
    "\n",
    "The model first concatenates user and restaurant embeddings, then adds bias terms before passing the result through a dense layer with 15 neurons and L2 regularization to help prevent overfitting. A dropout layer with a rate of 0.3 is applied to further mitigate overfitting. The output layer uses a sigmoid activation to generate ratings between 0 and 1, which are then scaled to the range between min_rating and max_rating. The model is compiled with the SGD optimizer and MSE loss, evaluating performance using RMSE. It is trained for 20 epochs with a batch size of 256, and validation is conducted on X_test and y_test.\n",
    "\n",
    "Here’s a summary of the training results for the deep neural network model over 20 epochs:\n",
    "\n",
    "- Epoch 1: The initial training RMSE is 1.4413, and validation RMSE is 1.4232, indicating the model is starting to learn.\n",
    "\n",
    "- Epoch 5: Training RMSE improves to 1.1802, while validation RMSE is 1.3785, showing progress in reducing error.\n",
    "\n",
    "- Epoch 10: The model's RMSE is 1.0074 for training and 1.3851 for validation, indicating a reduction in training error but some fluctuation in validation performance.\n",
    "\n",
    "- Epoch 15: Training RMSE reaches 0.9267, and validation RMSE is 1.3883, suggesting the model is learning well but validation error shows some instability.\n",
    "\n",
    "- Epoch 20: Final training RMSE is 0.8703, with a validation RMSE of 1.3940, reflecting a good fit on training data but slight overfitting or instability on validation data.\n",
    "\n",
    "\n",
    "**Tuned model 2**\n",
    "\n",
    "The model concatenates user and restaurant embeddings and adds bias terms before passing the result through a dense layer with 10 neurons and ReLU activation. A dropout layer with a 0.6 rate is used to prevent overfitting. The output layer, with a sigmoid activation, produces ratings between 0 and 1, which are then scaled to the desired rating range. The model is compiled using the SGD optimizer and MSE loss, and it is trained for 20 epochs with a batch size of 256, validated on X_test and y_test.\n",
    "\n",
    "Here’s a summary of the training results for the new model:\n",
    "\n",
    "- Initial Performance: At the beginning of training, the model had a root mean squared error (RMSE) of 1.2586 on the training set and 1.3816 on the validation set.\n",
    "\n",
    "- Improvement Trend: RMSE steadily improved over epochs, reaching 0.9310 on the training set by epoch 20, but the validation RMSE remained relatively high at 1.4233.\n",
    "\n",
    "- Validation Performance: The validation loss and RMSE showed less improvement compared to the training set, indicating potential overfitting or issues with generalization.\n",
    "\n",
    "- Dropout Effect: Despite increasing the dropout rate to 0.6, which typically helps with regularization, the model did not significantly outperform earlier versions, suggesting the need for additional tuning or adjustments.\n",
    "\n",
    "- Epoch Duration: Each epoch took around 220 to 270 seconds to complete, with a slight increase in training time towards the end, likely due to the model complexity and size of the dataset.\n",
    "\n",
    "In conclusion,the third model has further overfitted the training data as it has high validation score and low training score.\n",
    "Therefore our best neural model is baseline model which has a validation score of 1.3179.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the SVD and the best peforming DNN\n",
    "\n",
    "- **Initial Performance**: The DNN starts with a higher RMSE of 1.5015 compared to the SVD model's mean RMSE of 1.217, indicating the DNN requires training to optimize.\n",
    "\n",
    "\n",
    "- **Improvement Over Time**: The DNN shows a notable reduction in RMSE, ending at 1.4409 by epoch 10, yet this is still higher than the SVD model’s best RMSE of 1.2171.\n",
    "\n",
    "\n",
    "- **Validation Performance**: The validation RMSE for the DNN decreases from 1.5007 to 1.4512, which remains higher than the SVD model's mean RMSE.\n",
    "\n",
    "\n",
    "- **Consistency**: The SVD model's RMSE is consistent across folds, whereas the DNN improves gradually, reflecting learning over epochs.\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Despite improvements, the DNN’s final RMSE is higher than the SVD model's, suggesting that the SVD model performs better with this dataset and may require less tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECOMMENDATIONS\n",
    "\n",
    "1) *Finding: Most common ratings are 4.0 and 3.5 stars, indicating general satisfaction.*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Filter Out Lower Ratings**\n",
    "\n",
    "Although very poor ratings are rare, consider implementing a filter or threshold that minimizes the visibility of restaurants with ratings below a certain level (e.g., 3.0 stars). This will help ensure users are less likely to encounter options with significant negative feedback.\n",
    "\n",
    "- **Include User Preferences**\n",
    "\n",
    "Allow users to set preferences for certain rating ranges, so they can prioritize restaurants that align with their expectations. For example, a user might prefer options that are rated above 3.5 stars.\n",
    "\n",
    "- **Feature High-Rated Restaurants Prominently**\n",
    " \n",
    "Highlight in Recommendations: Ensure that top-rated restaurants are prominently featured in recommendations, especially for users seeking highly-rated options.\n",
    "\n",
    "\n",
    "2) *Finding: Pennsylvania (PA), Florida (FL) and Tennessee (TN) boast the highest numbers of restaurants among the states*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "\n",
    "- **Prioritize Major Markets for Data**\n",
    "\n",
    "Focus on PA, FL, and TN: Since these states have the highest number of restaurants, ensure that your dataset is comprehensive and up-to-date for these regions. Consider implementing advanced filtering options and more detailed reviews for users in these states, as there’s likely to be higher demand and a wider variety of choices.\n",
    "\n",
    "\n",
    "- **Tailor Recommendations to Market Size**\n",
    "\n",
    "Adjust the depth of recommendations based on the density of restaurants in the region. For major markets (like PA, FL, TN), provide more granular and diverse recommendations. For regions with fewer options (like NC, CO, HI, MT), focus on quality over quantity and highlight the best available choices.\n",
    "\n",
    "\n",
    "- **Offer Diverse Data Insights**\n",
    "\n",
    "Highlight Popular and Hidden Gems: In high-density areas, showcase a mix of popular spots and lesser-known but highly rated restaurants. In less populated states, emphasize unique and standout options to ensure users still have great dining experiences despite fewer choices.\n",
    "\n",
    "3) *Finding: 66.9% are Open Restaurants and 33.1% are Closed Restaurants:*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "\n",
    "- **Prioritize Accuracy by Filtering Closed Restaurants**\n",
    "\n",
    "Implement Robust Filters: Ensure that your system rigorously filters out closed restaurants. Regularly update your dataset to reflect the current operational status of restaurants, maintaining accuracy in your recommendations.\n",
    "\n",
    "\n",
    "- **Automated Alerts**\n",
    "\n",
    "Set up automated systems to flag and remove listings of closed restaurants as soon as possible.\n",
    "\n",
    "4) *Finding: McDonald's, Subway,  Taco Bell,Wendy's, Domino's Pizza, and Pizza Hut lead in frequency. This suggests a dominance of fast-food chains in the restaurant landscape.*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Leverage Popularity for Recommendations**\n",
    "\n",
    "Feature Popular Chains Prominently: Highlight McDonald's, Subway, Taco Bell, Wendy's, Domino's Pizza, and Pizza Hut in your recommendations. Users often seek familiar and reliable options, so showcasing these popular chains can enhance user satisfaction.\n",
    "\n",
    "\n",
    "- **Market and User Segmentation**\n",
    "\n",
    "Targeted Recommendations: Use segmentation to tailor recommendations based on user demographics or previous behavior. For example, users who frequently choose fast-food chains might see more recommendations for similar options.\n",
    "\n",
    "\n",
    "5) *Finding: Seasonal Trends: Frequent peaks between April and July, indicating increased customer engagement during this period.*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Incorporate Seasonality into Recommendations** \n",
    "\n",
    "Highlight Seasonal Popularity: During peak periods (April to July), increase the visibility of restaurants that typically see higher engagement. You might feature seasonal promotions or highlight trending restaurants during these months.\n",
    "\n",
    "6)  *Finding: Not all popular cuisines are included in the list of options*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Cuisine diversification**\n",
    "\n",
    "Feature variations of cuisines from different regions (e.g., Southern Italian vs. Northern Italian) to give users a more nuanced dining experience.\n",
    "\n",
    "- **Dietary Preferences**\n",
    "\n",
    "Include filters for dietary preferences and restrictions (e.g., vegetarian, vegan, gluten-free) to provide more personalized recommendations.\n",
    "\n",
    "- **Cuisine Exploration**\n",
    "\n",
    "Offer recommendations based on cuisine trends and new dining experiences. For example, feature emerging cuisines or new restaurant concepts that might interest adventurous diners.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
