{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "In this section we will create a recommendation system using the datasets to solve our main problem.\n",
    "There are different types of recomentation models, in this project we will focus on three types of recommentation systems\n",
    "\n",
    "* 1. Content-Based Recommender systems\n",
    "* 2. Collaborative Filtering Systems\n",
    "* 3. Deep Neural Networks\n",
    "\n",
    "Now, in each of these categories we will compare the different models and see which ones perform best. For validation and comparison we will use the RMSE (root mean squared error) metric, that is how far is the prediction from the true value.\n",
    "\n",
    "### 1. CONTENT BASED FILTERING\n",
    "\n",
    "By utilizing restaurant features such as types of cuisine they offer or if they have WiFi, Alcohol, Happy Hour, Noise Level, Restaurants Attire, Wheelchair Accessible, Restaurants TableService etc, we are able to use cosine similarity to recommend the  restaurants with the closest similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "# Text processing and NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Machine learning and model selection\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "# Deep learning with TensorFlow\n",
    "from tensorflow.keras import models, layers, optimizers, losses, regularizers, metrics\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "\n",
    "# Utility functions\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Custom imports\n",
    "from understanding import DataLoader, DataInfo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Cleaned Restaurant Informational Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38552 entries, 0 to 38551\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   business_id      38552 non-null  object \n",
      " 1   name             38552 non-null  object \n",
      " 2   address          38552 non-null  object \n",
      " 3   city             38552 non-null  object \n",
      " 4   state            38552 non-null  object \n",
      " 5   postal_code      38552 non-null  object \n",
      " 6   latitude         38552 non-null  float64\n",
      " 7   longitude        38552 non-null  float64\n",
      " 8   stars            38552 non-null  float64\n",
      " 9   review_count     38552 non-null  int64  \n",
      " 10  is_open          38552 non-null  int64  \n",
      " 11  attributes       38552 non-null  object \n",
      " 12  categories       38552 non-null  object \n",
      " 13  hours            38552 non-null  object \n",
      " 14  location         38552 non-null  object \n",
      " 15  attributes_true  38552 non-null  object \n",
      "dtypes: float64(3), int64(2), object(11)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Loading the restaurant data from the pickled file\n",
    "df = pd.read_pickle('pickled_files/restaurant_data.pkl')\n",
    "\n",
    "# Overview of dataset information to understand the features we require\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data to combine the needed features into one column\n",
    "    Returns a dataframe with the combined_features columns\n",
    "    \"\"\"\n",
    "    filtered_df=df.copy()\n",
    "    # Combining the features into one column\n",
    "    filtered_df['combined_features'] = (\n",
    "                                        filtered_df['attributes'] + \" \" +\n",
    "                                        filtered_df['attributes_true'] \n",
    "                                        )\n",
    "    # resetting the index\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "    # Return turns the filtered df\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization function\n",
    "def create_feature_vectors(df):\n",
    "    \"\"\"\n",
    "    Performing vectorization of the preprocessed categorical features \n",
    "    and combining with the numerical features\n",
    "    \"\"\"\n",
    "    # Vectorize the combined text features\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(df['combined_features'])\n",
    "    \n",
    "    # Combine the TF-IDF matrix with numerical columns\n",
    "    numerical_features = df[['stars']].values\n",
    "    combined_features = np.hstack((tfidf_matrix.toarray(), numerical_features))\n",
    "    \n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cosine similarity matrix we will now create a content-based recommendation system that offers recommendations to users based on the restaurant names or text words representing the specifications of their desired restaurant and attributes.\n",
    "\n",
    "We use the cosine similarity matrix to compare similarities between different restaurants and the customer's preferences, then pick the top n similar restaurants to recommend based on his/her input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation function\n",
    "def recommendation(df, state, name=None, category=None):\n",
    "    \"\"\"\n",
    "    Creates recommendation based on name or category/cuisine using cosine similarity and filtering\n",
    "    Returns a dataframe containing name, state, city, address, stars and categories\n",
    "    \"\"\"\n",
    "    preprocessed = preprocess(df)\n",
    "    \n",
    "    def cuisines(cuisine=None, state=state):\n",
    "        \"\"\"\n",
    "        Function to filter to get the recommendations based on cuisine input\n",
    "        \"\"\"\n",
    "        preprocessed=df[df[\"state\"]==state]\n",
    "        cuisine_df = preprocessed[preprocessed['categories'] == cuisine]\n",
    "        cuisine_df_sorted = cuisine_df.sort_values(by=[\"stars\", \"city\"], ascending=False)\n",
    "        return cuisine_df_sorted[['name', 'state', 'city', 'stars', 'address', 'categories']]\n",
    "    \n",
    "    if name:\n",
    "        if name not in preprocessed['name'].values:\n",
    "            raise ValueError(f\"Restaurant with name '{name}' not found in the filtered data.\")\n",
    "\n",
    "        # Finding the index of the restaurant name\n",
    "        idx = preprocessed[preprocessed['name'] == name].index[0]\n",
    "        exclude_names = [name]\n",
    "\n",
    "        # Locating the restaurant row in the preprocessed df \n",
    "        row_to_add = preprocessed.iloc[idx]\n",
    "        \n",
    "        # convering it to a df\n",
    "        row_to_add_df = pd.DataFrame([row_to_add])     \n",
    "        \n",
    "        #generating a df for only the state i want to recommend in\n",
    "        specific_state= preprocessed[preprocessed[\"state\"] == state]\n",
    "        \n",
    "        # concatinating it to the specific state df and reseting the index\n",
    "        specific_state = pd.concat([specific_state, row_to_add_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Finding the new index for the restaurant name\n",
    "        idx = specific_state[specific_state['name'] == name].index[0]\n",
    "        \n",
    "        # Creating feature vectors\n",
    "        combined_features = create_feature_vectors(specific_state)\n",
    "\n",
    "        # Finding the cosine similarity\n",
    "        cosine_sim = cosine_similarity(combined_features, combined_features)\n",
    "\n",
    "        # Finding the top indices of the restaurants to recommend\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        top_indices = [i[0] for i in sim_scores]  \n",
    "\n",
    "        # Finding the rows of the top recommended restaurants\n",
    "        recommended_restaurants = specific_state.iloc[top_indices]\n",
    "        recommended_restaurants = recommended_restaurants[~recommended_restaurants['name'].isin(exclude_names)]        \n",
    "\n",
    "        # Return a df with the required features\n",
    "        return recommended_restaurants[['name', 'state', 'city', 'stars', 'address','categories']].drop_duplicates(subset='name')[:20]\n",
    "    \n",
    "    elif category:\n",
    "        # Filter based on cuisine/cateogry\n",
    "        return cuisines(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content_based function uses content-based recommendation techniques to provide restaurant recommendations based on user input preferences, restaurant names, or user-defined text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>stars</th>\n",
       "      <th>address</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>El Torito Grill</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8650 Keystone Crossing</td>\n",
       "      <td>Mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Taste of China</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Whiteland</td>\n",
       "      <td>4.0</td>\n",
       "      <td>989 N US 31</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>Hong Kong Inn</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8079 E 38th St</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>Diarra's Cuisine</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2989 W 71st St, Ste 3</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>WB Pizza</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2290 W 86th St</td>\n",
       "      <td>American (Traditional)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name    state          city  stars                 address  \\\n",
       "1785   El Torito Grill  Indiana  Indianapolis    4.5  8650 Keystone Crossing   \n",
       "40      Taste of China  Indiana     Whiteland    4.0             989 N US 31   \n",
       "2133     Hong Kong Inn  Indiana  Indianapolis    4.0          8079 E 38th St   \n",
       "2062  Diarra's Cuisine  Indiana  Indianapolis    3.5   2989 W 71st St, Ste 3   \n",
       "458           WB Pizza  Indiana  Indianapolis    4.5          2290 W 86th St   \n",
       "\n",
       "                  categories  \n",
       "1785                 Mexican  \n",
       "40                   Chinese  \n",
       "2133                 Chinese  \n",
       "2062                 African  \n",
       "458   American (Traditional)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example recommendations based on state, name\n",
    "restaurants = recommendation(df, state=\"Indiana\",  name=\"Coup de Taco\")\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>stars</th>\n",
       "      <th>address</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19189</th>\n",
       "      <td>Greek’s Pizzeria-  Indianapolis</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1601 Columbia Ave</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30169</th>\n",
       "      <td>I Tre Mori</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8220 E 106th St, Ste 200</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35845</th>\n",
       "      <td>The Twisted Sicilian</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>Ciao by Villaggio</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Zionsville</td>\n",
       "      <td>4.5</td>\n",
       "      <td>40 S Main St</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21713</th>\n",
       "      <td>Convivio Italian Artisan Cuisine - Zionsville</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Zionsville</td>\n",
       "      <td>4.5</td>\n",
       "      <td>40 S Main St</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name    state          city  \\\n",
       "19189                Greek’s Pizzeria-  Indianapolis  Indiana  Indianapolis   \n",
       "30169                                     I Tre Mori  Indiana  Indianapolis   \n",
       "35845                           The Twisted Sicilian  Indiana  Indianapolis   \n",
       "12466                              Ciao by Villaggio  Indiana    Zionsville   \n",
       "21713  Convivio Italian Artisan Cuisine - Zionsville  Indiana    Zionsville   \n",
       "\n",
       "       stars                   address categories  \n",
       "19189    5.0         1601 Columbia Ave    Italian  \n",
       "30169    5.0  8220 E 106th St, Ste 200    Italian  \n",
       "35845    5.0                   Unknown    Italian  \n",
       "12466    4.5              40 S Main St    Italian  \n",
       "21713    4.5              40 S Main St    Italian  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example recommendations based on state, category/cuisine\n",
    "cuisines = recommendation(df, state=\"Indiana\",  category=\"Italian\")\n",
    "cuisines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLABORATIVE FILTERING MODELS\n",
    "\n",
    "\n",
    "Here the tasks related to building a collaborative filtering recommendation system using the Surprise library are undertaken for collaborative filtering by selecting the relevant columns, importing the Surprise library, initializing a Reader object to specify the data format, and then loading the data into a Surprise Dataset object for further analysis and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Cleaned User Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER DATASET INFORMATION\n",
      "========================================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4724684 entries, 0 to 4724683\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   user_id      object\n",
      " 1   business_id  object\n",
      " 2   stars        int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 108.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Loading the users csv file\n",
    "users_data= pd.read_csv(\"data/users (1).csv\")\n",
    "\n",
    "# Summary information on the user review data\n",
    "print(f'\\nUSER DATASET INFORMATION\\n' + '=='*20 + '\\n')\n",
    "users_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "      <th>location</th>\n",
       "      <th>attributes_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>Zaika</td>\n",
       "      <td>2481 Grant Ave</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.02508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Caters': 'True', 'Ambience': \"{'romantic': F...</td>\n",
       "      <td>Halal</td>\n",
       "      <td>{'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...</td>\n",
       "      <td>State:Pennsylvania, City:Philadelphia, Address...</td>\n",
       "      <td>Caters Ambience_casual BikeParking Restaurants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>Zaika</td>\n",
       "      <td>2481 Grant Ave</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.02508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Caters': 'True', 'Ambience': \"{'romantic': F...</td>\n",
       "      <td>Pakistani</td>\n",
       "      <td>{'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...</td>\n",
       "      <td>State:Pennsylvania, City:Philadelphia, Address...</td>\n",
       "      <td>Caters Ambience_casual BikeParking Restaurants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>Zaika</td>\n",
       "      <td>2481 Grant Ave</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.02508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Caters': 'True', 'Ambience': \"{'romantic': F...</td>\n",
       "      <td>Indian</td>\n",
       "      <td>{'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...</td>\n",
       "      <td>State:Pennsylvania, City:Philadelphia, Address...</td>\n",
       "      <td>Caters Ambience_casual BikeParking Restaurants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kSMOJwJXuEUqzfmuFncK4A</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>2</td>\n",
       "      <td>Zaika</td>\n",
       "      <td>2481 Grant Ave</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.02508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Caters': 'True', 'Ambience': \"{'romantic': F...</td>\n",
       "      <td>Halal</td>\n",
       "      <td>{'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...</td>\n",
       "      <td>State:Pennsylvania, City:Philadelphia, Address...</td>\n",
       "      <td>Caters Ambience_casual BikeParking Restaurants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kSMOJwJXuEUqzfmuFncK4A</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>2</td>\n",
       "      <td>Zaika</td>\n",
       "      <td>2481 Grant Ave</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>19114</td>\n",
       "      <td>40.079848</td>\n",
       "      <td>-75.02508</td>\n",
       "      <td>4.0</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Caters': 'True', 'Ambience': \"{'romantic': F...</td>\n",
       "      <td>Pakistani</td>\n",
       "      <td>{'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...</td>\n",
       "      <td>State:Pennsylvania, City:Philadelphia, Address...</td>\n",
       "      <td>Caters Ambience_casual BikeParking Restaurants...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  stars_x   name  \\\n",
       "0  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA        5  Zaika   \n",
       "1  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA        5  Zaika   \n",
       "2  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA        5  Zaika   \n",
       "3  kSMOJwJXuEUqzfmuFncK4A  kxX2SOes4o-D3ZQBkiMRfA        2  Zaika   \n",
       "4  kSMOJwJXuEUqzfmuFncK4A  kxX2SOes4o-D3ZQBkiMRfA        2  Zaika   \n",
       "\n",
       "          address          city         state postal_code   latitude  \\\n",
       "0  2481 Grant Ave  Philadelphia  Pennsylvania       19114  40.079848   \n",
       "1  2481 Grant Ave  Philadelphia  Pennsylvania       19114  40.079848   \n",
       "2  2481 Grant Ave  Philadelphia  Pennsylvania       19114  40.079848   \n",
       "3  2481 Grant Ave  Philadelphia  Pennsylvania       19114  40.079848   \n",
       "4  2481 Grant Ave  Philadelphia  Pennsylvania       19114  40.079848   \n",
       "\n",
       "   longitude  stars_y  review_count  is_open  \\\n",
       "0  -75.02508      4.0           181        1   \n",
       "1  -75.02508      4.0           181        1   \n",
       "2  -75.02508      4.0           181        1   \n",
       "3  -75.02508      4.0           181        1   \n",
       "4  -75.02508      4.0           181        1   \n",
       "\n",
       "                                          attributes categories  \\\n",
       "0  {'Caters': 'True', 'Ambience': \"{'romantic': F...      Halal   \n",
       "1  {'Caters': 'True', 'Ambience': \"{'romantic': F...  Pakistani   \n",
       "2  {'Caters': 'True', 'Ambience': \"{'romantic': F...     Indian   \n",
       "3  {'Caters': 'True', 'Ambience': \"{'romantic': F...      Halal   \n",
       "4  {'Caters': 'True', 'Ambience': \"{'romantic': F...  Pakistani   \n",
       "\n",
       "                                               hours  \\\n",
       "0  {'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...   \n",
       "1  {'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...   \n",
       "2  {'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...   \n",
       "3  {'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...   \n",
       "4  {'Tuesday': '11:0-21:0', 'Wednesday': '11:0-21...   \n",
       "\n",
       "                                            location  \\\n",
       "0  State:Pennsylvania, City:Philadelphia, Address...   \n",
       "1  State:Pennsylvania, City:Philadelphia, Address...   \n",
       "2  State:Pennsylvania, City:Philadelphia, Address...   \n",
       "3  State:Pennsylvania, City:Philadelphia, Address...   \n",
       "4  State:Pennsylvania, City:Philadelphia, Address...   \n",
       "\n",
       "                                     attributes_true  \n",
       "0  Caters Ambience_casual BikeParking Restaurants...  \n",
       "1  Caters Ambience_casual BikeParking Restaurants...  \n",
       "2  Caters Ambience_casual BikeParking Restaurants...  \n",
       "3  Caters Ambience_casual BikeParking Restaurants...  \n",
       "4  Caters Ambience_casual BikeParking Restaurants...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the two datasets into one using the business_id primary key\n",
    "\n",
    "data=pd.merge(left=users_data, right=df, how='inner', on='business_id')\n",
    "\n",
    "# previewing the new merge dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns\n",
    "\n",
    "Renaming the **stars_x** and **stars_y** columns into **rating** and **b/s_rating** columns for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'stars_x':'b/s_rating', 'stars_y':'rating'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4404612 entries, 0 to 4404611\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   user_id          object \n",
      " 1   business_id      object \n",
      " 2   b/s_rating       int64  \n",
      " 3   name             object \n",
      " 4   address          object \n",
      " 5   city             object \n",
      " 6   state            object \n",
      " 7   postal_code      object \n",
      " 8   latitude         float64\n",
      " 9   longitude        float64\n",
      " 10  rating           float64\n",
      " 11  review_count     int64  \n",
      " 12  is_open          int64  \n",
      " 13  attributes       object \n",
      " 14  categories       object \n",
      " 15  hours            object \n",
      " 16  location         object \n",
      " 17  attributes_true  object \n",
      "dtypes: float64(3), int64(3), object(12)\n",
      "memory usage: 604.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First , we will model a baseline SVD() model using the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  1134020 \n",
      "\n",
      "Number of Restaurants:  29354\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVD, accuracy, NormalPredictor\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "\n",
    " #selecting specific columns that are relevant for collaborative filtering models\n",
    "new_df = data[['user_id', 'business_id', 'rating']]\n",
    "\n",
    "# using Reader() from surprise module to convert dataframe into surprise dataformat\n",
    "# instantiating a readerobject\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# using the reader to read the trainset\n",
    "data_2 = Dataset.load_from_df(new_df,reader)\n",
    "\n",
    "dataset = data_2.build_full_trainset()\n",
    "\n",
    "print('Number of users: ', dataset.n_users, '\\n')\n",
    "print('Number of Restaurants: ', dataset.n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8108282260080057"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split \n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data_2, test_size=0.25)\n",
    "\n",
    "# Initialize the SVD algorithm\n",
    "model = NormalPredictor()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Compute rmse\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09089867290929593"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the SVD algorithm\n",
    "model = SVD()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross-validate to get the test rmse scores for 5 splits\n",
    "results=cross_validate(model, data_2, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "for values in results.items():\n",
    "    print(values)\n",
    "print(\"-------------------------\")\n",
    "print(\"Mean RMSE: \",results['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the GridSearchCv we will tune the SVD model in order to improve the training RMSE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary params with hyperparameter values to be tested\n",
    "params = {'n_factors': [20, 50, 100], # number of factors for matrix factorization\n",
    "         'reg_all': [0.02, 0.05, 0.1]} # regularization term\n",
    "# create a GridSearchCV object 'g_s_svd' for hyperparameter tuning\n",
    "g_s_svd = GridSearchCV(SVD,param_grid=params,n_jobs=-1) # specify the algorithm (SVD) to be tuned\n",
    "# fit the GridSearchCV object to the data to find the best hyperparameters\n",
    "g_s_svd.fit(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform hyperparameter tuning for the SVD collaborative filtering model using grid search and cross-validation. It tests different values of the number of latent factors (n_factors) and the regularization term (reg_all) to find the combination that results in the best model performance. The final best hyperparameters can be accessed from the g_s_svd object for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.2171066185237724, 'mae': 0.9478990451899783}\n",
      "{'rmse': {'n_factors': 100, 'reg_all': 0.02}, 'mae': {'n_factors': 100, 'reg_all': 0.02}}\n"
     ]
    }
   ],
   "source": [
    "print(g_s_svd.best_score)\n",
    "print(g_s_svd.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE value for the optimized SVD model is approximately 1.254, indicating the model's average prediction error in terms of user ratings. Lower RMSE values are desirable as they signify better predictive accuracy.                              \n",
    "The MAE value for the optimized SVD model is approximately 1.01, representing the average absolute difference between predicted and actual user ratings. A lower MAE indicates improved prediction accuracy.                                            \n",
    "The best-performing hyperparameter values are as follows:                       \n",
    "1) For RMSE, the optimal hyperparameters are 'n_factors' = 20 and 'reg_all' = 0.05.\n",
    "2) For MAE, the optimal hyperparameters are 'n_factors' = 20 and 'reg_all' = 0.02.   \n",
    "These results indicate that the SVD collaborative filtering model, when configured with these hyperparameters, provides a relatively low prediction error and is well-suited for making personalized recommendations based on user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f21e74f10d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# created an instance of the SVD model with specified hyperparameters\n",
    "svd = SVD(n_factors= 20, reg_all=0.02)\n",
    "# fit the SVD model to the dataset\n",
    "svd.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code we just did initializes an SVD model with specific hyperparameters and then trains the model on the provided dataset. The trained SVD model can be used for various tasks, such as making personalized recommendations based on user-item interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='15', iid='Pns2l4eNsfO8kk83dixA6A', r_ui=None, est=3.8457184924429697, details={'was_impossible': False})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the model, we'll try and make a rating prediction of user 15, on restaurant with id \"Pns2l4eNsfO8kk83dixA6A\"\n",
    "svd.predict(\"15\", \"Pns2l4eNsfO8kk83dixA6A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below  allows a user to interactively rate restaurants by providing their ratings for a specified number of restaurants, and it collects this information in a list for further analysis or use in a recommendation system. The code also considers the restaurant category for selecting restaurants to rate if a category is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ratings(df, num_samples=5):\n",
    "    sampled_restaurants = df.sample(n=num_samples)\n",
    "    ratings = []\n",
    "    for _, row in sampled_restaurants.iterrows():\n",
    "        restaurant_id = row['business_id']\n",
    "        print(f\"Please rate {row['name']} on a scale of 1 to 5:\")\n",
    "        rating = int(input())\n",
    "        ratings.append((restaurant_id, rating))\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_restaurants(user_id, rated_restaurants, all_restaurants_df=trial, state=None):\n",
    "       \n",
    "    # Get all restaurant IDs\n",
    "    all_restaurants_df= all_restaurants_df[all_restaurants_df[\"state\"]==state]\n",
    "    all_restaurant_ids = all_restaurants_df['business_id'].unique()\n",
    "\n",
    "    # Filter out the restaurants that the user has already rated\n",
    "    unrated_restaurants = [rid for rid in all_restaurant_ids if rid not in [rid for rid, _ in rated_restaurants]]\n",
    "\n",
    "    # Predict ratings for all unrated restaurants\n",
    "    predictions = [svd.predict(user_id, rid) for rid in unrated_restaurants]\n",
    "    \n",
    "\n",
    "    # Create a DataFrame for the predictions\n",
    "    pred_df = pd.DataFrame({\n",
    "        'business_id': [pred.iid for pred in predictions],\n",
    "        'predicted_rating': [pred.est for pred in predictions]\n",
    "    })\n",
    "\n",
    "    # Merge with the original restaurants DataFrame to get more information\n",
    "    recommendations = pred_df.merge(all_restaurants_df, on='business_id', how='left')\n",
    "\n",
    "    # Sort by predicted rating and get top recommendations\n",
    "    recommendations = recommendations.sort_values(by='predicted_rating', ascending=False)\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_businesses(user_id, n=5):\n",
    "    # Get all unique business IDs\n",
    "    all_business_ids = df_collab['business_id'].unique()\n",
    "    \n",
    "    # Predict ratings for all businesses the user hasn't rated yet\n",
    "    user_rated_businesses = df_collab[df_collab['user_id'] == user_id]['business_id']\n",
    "    recommendations = []\n",
    "    \n",
    "    for business_id in all_business_ids:\n",
    "        if business_id not in user_rated_businesses.values:\n",
    "            pred = model.predict(user_id, business_id)\n",
    "            recommendations.append((business_id, pred.est))\n",
    "    \n",
    "    # Sort by estimated rating and return top-n\n",
    "    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ratings from the user\n",
    "print(\"You will be asked to rate 5 random restaurants.\")\n",
    "user_ratings = collect_ratings(restaurant_data)\n",
    "\n",
    "user_id = 'user_1'\n",
    "recommended_restaurants = recommend_restaurants(user_id=user_id, rated_restaurants=user_ratings, state= \"Pennsylvania\").drop_duplicates(subset='name')[:20]\n",
    "recommended_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your restaurant data\n",
    "df = pd.read_csv('data/filtered_restaurants_data.csv')\n",
    "\n",
    "\n",
    "# Your Yelp API key\n",
    "API_KEY = 'QO9XAZfxn80KoHc2rPOj9iEhWK2r8EJXfLNH_Q1F2O04d3XpAvdxFiX0Bz1wKge_hR0IMLsbsn2-ObSe0uTx5EWttuS_Yy_6wYvew5D0GXBGru_BV2OkyQDUlQOyZnYx'\n",
    "\n",
    "# Yelp Business Endpoint\n",
    "YELP_BUSINESS_URL = \"https://api.yelp.com/v3/businesses/\"\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "}\n",
    "\n",
    "def get_business_image_urls(business_id):\n",
    "    response = requests.get(f'{YELP_BUSINESS_URL}{business_id}', headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        business_data = response.json()\n",
    "        # Extract the image URLs\n",
    "        image_urls = business_data.get('photos', [])\n",
    "        return image_urls\n",
    "    elif response.status_code == 429:\n",
    "        st.error(\"Rate limit exceeded. Please try again later.\")\n",
    "    else:\n",
    "        st.error(f\"Failed to retrieve data for Business ID: {business_id}, Status Code: {response.status_code}\")\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_business_image_urls('Ep_jh1Pt4Ggyla21f-BQcQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a Keras deep neural network to implement a recommendation system and try to improve our RMSE scores by using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are going to encode the user_id and business_id features into numeric integers in preparation for the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users:  220872\n",
      "Number of Restaurants:  31834\n"
     ]
    }
   ],
   "source": [
    "# Encoding the user_id column\n",
    "user_encoder = LabelEncoder()                                    # instantiating the encoder\n",
    "data['userId'] = user_encoder.fit_transform(data.user_id.values) # fitting and transforming the encoder to our column\n",
    "n_users=data['userId'].nunique()                                 # assigning the number of users to n_user vaiable\n",
    "print(\"Number of Users: \",n_users)\n",
    "\n",
    "# Encoding the business_id column\n",
    "item_encoder = LabelEncoder()                                          # instantiating the encoder\n",
    "data['restId'] = user_encoder.fit_transform(data.business_id.values)   # fitting and transforming the encoder to our column\n",
    "n_rests = data['restId'].nunique()                                  # assigning the number of restaurants to n_rests vaiable\n",
    "print(\"Number of Restaurants: \",n_rests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Splitting the data into training and testing sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428157, 2) (428157,)\n",
      "(107040, 2) (107040,)\n"
     ]
    }
   ],
   "source": [
    "# subsetting the x variable\n",
    "X = data[['userId', 'restId']].values\n",
    "# subsetting the y variable\n",
    "y = data['rating'].values\n",
    "\n",
    "# creating the train test splits and stratifying on basis of the y values \n",
    "# because of the uneven nature of the rating counts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calculate the minimum and maximum ratings, which will be used to scale the output of the neural network later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum rating\n",
    "min_rating = min(data['rating'])\n",
    "max_rating = max(data['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The predicted ratings is calculated by multiplying the user and restaurant embeddings, then adding the user and restaurant bias. Therefore were are going to create user and restaurant embeddings together with bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of latent factors\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Defining user embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User embeddings\n",
    "\n",
    "# user input layer\n",
    "user = layers.Input(shape=(1,))\n",
    "\n",
    "# Embedding layer for calculating user latent factors of size 50\n",
    "user_emb = layers.Embedding(n_users, embedding_size, embeddings_regularizer=regularizers.l2(1e-6))(user)\n",
    "\n",
    "# Reshaping the layer to flatten the embedding vector.\n",
    "user_emb = layers.Reshape((embedding_size,))(user_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Defining user bias, and reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User bias\n",
    "\n",
    "# Embedding layer\n",
    "user_bias = layers.Embedding(n_users, 1, embeddings_regularizer=regularizers.l2(1e-6))(user)\n",
    "\n",
    "# Reshapin the user bias layer\n",
    "user_bias = layers.Reshape((1,))(user_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Defining restaurants embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restaurant embeddings\n",
    "\n",
    "# Input layer\n",
    "restaurant= layers.Input(shape=(1,))\n",
    "\n",
    "# Embedding layer\n",
    "rest_emb = layers.Embedding(n_rests, embedding_size, embeddings_regularizer=regularizers.l2(1e-6))(restaurant)\n",
    "\n",
    "# Reshape layer\n",
    "rest_emb = layers.Reshape((embedding_size,))(rest_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Defining restaurant bias, and reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurant bias\n",
    "\n",
    "# Embedding layer\n",
    "rest_bias = layers.Embedding(n_rests, 1, embeddings_regularizer=regularizers.l2(1e-6))(restaurant)\n",
    "\n",
    "# Reshape layer\n",
    "rest_bias = layers.Reshape((1,))(rest_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After defining the embedding and bias layers, the predicted rating is calculated by dot product of the user and restaurant embeddings and then adding the bias values in order to get more accurate ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product of the user and restaurant embeddings\n",
    "rating = layers.Concatenate()([user_emb, rest_emb])\n",
    "\n",
    "# Add biases to the ratings\n",
    "# Adding the user and restaurant bias to the predicted rating\n",
    "rating = layers.Add()([rating, user_bias, rest_bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We move on to pass the calculated rating to layers of dense networks and finally converting the rating score from binary values into a range of 1-5. \n",
    "\n",
    "We create our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 146ms/step - loss: 2.2653 - root_mean_squared_error: 1.5015 - val_loss: 2.2630 - val_root_mean_squared_error: 1.5007\n",
      "Epoch 2/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 148ms/step - loss: 2.2557 - root_mean_squared_error: 1.4983 - val_loss: 2.2616 - val_root_mean_squared_error: 1.5003\n",
      "Epoch 3/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 138ms/step - loss: 2.2602 - root_mean_squared_error: 1.4998 - val_loss: 2.2623 - val_root_mean_squared_error: 1.5005\n",
      "Epoch 4/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 185ms/step - loss: 2.2639 - root_mean_squared_error: 1.5011 - val_loss: 2.2614 - val_root_mean_squared_error: 1.5002\n",
      "Epoch 5/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 157ms/step - loss: 2.2591 - root_mean_squared_error: 1.4994 - val_loss: 2.2607 - val_root_mean_squared_error: 1.5000\n",
      "Epoch 6/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 147ms/step - loss: 2.2514 - root_mean_squared_error: 1.4969 - val_loss: 2.2590 - val_root_mean_squared_error: 1.4994\n",
      "Epoch 7/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 140ms/step - loss: 2.2587 - root_mean_squared_error: 1.4993 - val_loss: 2.2549 - val_root_mean_squared_error: 1.4980\n",
      "Epoch 8/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 148ms/step - loss: 2.2457 - root_mean_squared_error: 1.4950 - val_loss: 2.2322 - val_root_mean_squared_error: 1.4905\n",
      "Epoch 9/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 180ms/step - loss: 2.1951 - root_mean_squared_error: 1.4780 - val_loss: 2.1834 - val_root_mean_squared_error: 1.4740\n",
      "Epoch 10/10\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 187ms/step - loss: 2.0870 - root_mean_squared_error: 1.4409 - val_loss: 2.1166 - val_root_mean_squared_error: 1.4512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f21fe2e3950>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# first dense layer of 30 nodes with relu activation\n",
    "rating = layers.Dense(30, activation='relu')(rating)\n",
    "\n",
    "# second dense layer of 15 nodes\n",
    "rating = layers.Dense(15, activation='relu')(rating)\n",
    "\n",
    "# output layer with one node that produces values between 0 and 1 due to the sigmoid activation\n",
    "rating = layers.Dense(1, activation='sigmoid')(rating)\n",
    "# rating= layers.Dense(5, activation='softmax')(rating)\n",
    "\n",
    "# Scales the predicted ratings to a range of 1 - 5\n",
    "rating = layers.Lambda(lambda x:x*(max_rating - min_rating) + min_rating)(rating)\n",
    "\n",
    "\n",
    "# Baseline Model \n",
    "baseline_model = models.Model([user, restaurant], rating)\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile( optimizer='sgd', loss='mse',  metrics=[metrics.RootMeanSquaredError()])\n",
    "\n",
    "# training the model\n",
    "baseline_model .fit(x=[X_train[:,0], X_train[:,1]], y=y_train,\n",
    "                    batch_size=256, \n",
    "                    epochs=10, \n",
    "                    verbose=1,\n",
    "                    validation_data=([X_test[:,0], X_test[:,1]], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our baseline model, does not overfit since the training RMSE score and the validation scores are not far off. We then proceed to tune the model in order to get better rmse scores, by reducing the model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 184ms/step - loss: 2.1169 - root_mean_squared_error: 1.4413 - val_loss: 2.0627 - val_root_mean_squared_error: 1.4232\n",
      "Epoch 2/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 153ms/step - loss: 1.8676 - root_mean_squared_error: 1.3530 - val_loss: 2.0022 - val_root_mean_squared_error: 1.4022\n",
      "Epoch 3/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 188ms/step - loss: 1.7143 - root_mean_squared_error: 1.2955 - val_loss: 1.9576 - val_root_mean_squared_error: 1.3865\n",
      "Epoch 4/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 180ms/step - loss: 1.5578 - root_mean_squared_error: 1.2340 - val_loss: 1.9376 - val_root_mean_squared_error: 1.3795\n",
      "Epoch 5/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 134ms/step - loss: 1.4274 - root_mean_squared_error: 1.1802 - val_loss: 1.9344 - val_root_mean_squared_error: 1.3785\n",
      "Epoch 6/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 119ms/step - loss: 1.3002 - root_mean_squared_error: 1.1252 - val_loss: 1.9345 - val_root_mean_squared_error: 1.3787\n",
      "Epoch 7/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 145ms/step - loss: 1.2128 - root_mean_squared_error: 1.0859 - val_loss: 1.9420 - val_root_mean_squared_error: 1.3817\n",
      "Epoch 8/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 146ms/step - loss: 1.1415 - root_mean_squared_error: 1.0529 - val_loss: 1.9400 - val_root_mean_squared_error: 1.3812\n",
      "Epoch 9/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 160ms/step - loss: 1.0845 - root_mean_squared_error: 1.0258 - val_loss: 1.9468 - val_root_mean_squared_error: 1.3839\n",
      "Epoch 10/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 151ms/step - loss: 1.0464 - root_mean_squared_error: 1.0074 - val_loss: 1.9494 - val_root_mean_squared_error: 1.3851\n",
      "Epoch 11/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 157ms/step - loss: 1.0143 - root_mean_squared_error: 0.9917 - val_loss: 1.9519 - val_root_mean_squared_error: 1.3863\n",
      "Epoch 12/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 163ms/step - loss: 0.9740 - root_mean_squared_error: 0.9716 - val_loss: 1.9518 - val_root_mean_squared_error: 1.3865\n",
      "Epoch 13/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 157ms/step - loss: 0.9467 - root_mean_squared_error: 0.9578 - val_loss: 1.9566 - val_root_mean_squared_error: 1.3885\n",
      "Epoch 14/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 147ms/step - loss: 0.9188 - root_mean_squared_error: 0.9436 - val_loss: 1.9643 - val_root_mean_squared_error: 1.3915\n",
      "Epoch 15/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 138ms/step - loss: 0.8866 - root_mean_squared_error: 0.9267 - val_loss: 1.9547 - val_root_mean_squared_error: 1.3883\n",
      "Epoch 16/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 139ms/step - loss: 0.8685 - root_mean_squared_error: 0.9172 - val_loss: 1.9573 - val_root_mean_squared_error: 1.3895\n",
      "Epoch 17/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 150ms/step - loss: 0.8411 - root_mean_squared_error: 0.9025 - val_loss: 1.9578 - val_root_mean_squared_error: 1.3899\n",
      "Epoch 18/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 149ms/step - loss: 0.8167 - root_mean_squared_error: 0.8893 - val_loss: 1.9564 - val_root_mean_squared_error: 1.3896\n",
      "Epoch 19/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 149ms/step - loss: 0.8051 - root_mean_squared_error: 0.8830 - val_loss: 1.9670 - val_root_mean_squared_error: 1.3936\n",
      "Epoch 20/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 150ms/step - loss: 0.7822 - root_mean_squared_error: 0.8703 - val_loss: 1.9675 - val_root_mean_squared_error: 1.3940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f21f98c1b50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rating = layers.Concatenate()([user_emb, rest_emb])\n",
    "rating = layers.Add()([rating, user_bias, rest_bias])\n",
    "\n",
    "# redusing the first dense layer into 15 neurons and adding a l2 regularization\n",
    "rating = layers.Dense(15, activation='relu',kernel_regularizer=regularizers.l2(1e-3))(rating)\n",
    "# creating a dropout layer\n",
    "rating = layers.Dropout(0.3)(rating)\n",
    "# output layer\n",
    "rating = layers.Dense(1, activation='sigmoid')(rating)\n",
    "#convertion of output rating\n",
    "rating = layers.Lambda(lambda x:x*(max_rating - min_rating) + min_rating)(rating)\n",
    "\n",
    "model_1 = models.Model([user, restaurant], rating)\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile( optimizer='sgd', loss='mse',  metrics=[metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Train the model\n",
    "model_1.fit(x=[X_train[:,0], X_train[:,1]], y=y_train,\n",
    "            batch_size=256,\n",
    "            epochs=20, \n",
    "            verbose=1,\n",
    "            validation_data=([X_test[:,0], X_test[:,1]], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The second model has performed worse than the first with a higher rmse score and the model is overfitting the training data i.e it has a good train score but poor validation score.\n",
    "\n",
    "we will try and simplify the model further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 141ms/step - loss: 1.6052 - root_mean_squared_error: 1.2586 - val_loss: 1.9195 - val_root_mean_squared_error: 1.3816\n",
      "Epoch 2/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 133ms/step - loss: 1.1682 - root_mean_squared_error: 1.0758 - val_loss: 1.9942 - val_root_mean_squared_error: 1.4083\n",
      "Epoch 3/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 134ms/step - loss: 1.1277 - root_mean_squared_error: 1.0568 - val_loss: 2.0187 - val_root_mean_squared_error: 1.4170\n",
      "Epoch 4/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 133ms/step - loss: 1.0749 - root_mean_squared_error: 1.0315 - val_loss: 2.0266 - val_root_mean_squared_error: 1.4198\n",
      "Epoch 5/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 133ms/step - loss: 1.0261 - root_mean_squared_error: 1.0076 - val_loss: 2.0196 - val_root_mean_squared_error: 1.4173\n",
      "Epoch 6/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 131ms/step - loss: 0.9996 - root_mean_squared_error: 0.9943 - val_loss: 2.0127 - val_root_mean_squared_error: 1.4149\n",
      "Epoch 7/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 131ms/step - loss: 0.9819 - root_mean_squared_error: 0.9854 - val_loss: 2.0153 - val_root_mean_squared_error: 1.4158\n",
      "Epoch 8/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 132ms/step - loss: 0.9672 - root_mean_squared_error: 0.9779 - val_loss: 2.0235 - val_root_mean_squared_error: 1.4187\n",
      "Epoch 9/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 130ms/step - loss: 0.9527 - root_mean_squared_error: 0.9705 - val_loss: 2.0196 - val_root_mean_squared_error: 1.4173\n",
      "Epoch 10/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 131ms/step - loss: 0.9368 - root_mean_squared_error: 0.9622 - val_loss: 2.0274 - val_root_mean_squared_error: 1.4200\n",
      "Epoch 11/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 132ms/step - loss: 0.9351 - root_mean_squared_error: 0.9614 - val_loss: 2.0260 - val_root_mean_squared_error: 1.4196\n",
      "Epoch 12/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 131ms/step - loss: 0.9181 - root_mean_squared_error: 0.9525 - val_loss: 2.0289 - val_root_mean_squared_error: 1.4206\n",
      "Epoch 13/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 132ms/step - loss: 0.9169 - root_mean_squared_error: 0.9519 - val_loss: 2.0294 - val_root_mean_squared_error: 1.4208\n",
      "Epoch 14/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 132ms/step - loss: 0.9117 - root_mean_squared_error: 0.9491 - val_loss: 2.0364 - val_root_mean_squared_error: 1.4232\n",
      "Epoch 15/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 137ms/step - loss: 0.9107 - root_mean_squared_error: 0.9486 - val_loss: 2.0373 - val_root_mean_squared_error: 1.4235\n",
      "Epoch 16/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 130ms/step - loss: 0.8983 - root_mean_squared_error: 0.9420 - val_loss: 2.0377 - val_root_mean_squared_error: 1.4237\n",
      "Epoch 17/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 131ms/step - loss: 0.8937 - root_mean_squared_error: 0.9396 - val_loss: 2.0325 - val_root_mean_squared_error: 1.4218\n",
      "Epoch 18/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 131ms/step - loss: 0.8919 - root_mean_squared_error: 0.9386 - val_loss: 2.0342 - val_root_mean_squared_error: 1.4224\n",
      "Epoch 19/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 133ms/step - loss: 0.8839 - root_mean_squared_error: 0.9343 - val_loss: 2.0392 - val_root_mean_squared_error: 1.4242\n",
      "Epoch 20/20\n",
      "\u001b[1m1673/1673\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 132ms/step - loss: 0.8778 - root_mean_squared_error: 0.9310 - val_loss: 2.0367 - val_root_mean_squared_error: 1.4233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f22018e22d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rating = layers.Concatenate()([user_emb, rest_emb])\n",
    "# Adds the user and restaurant embedding to the dot product of the embeddings\n",
    "rating = layers.Add()([rating, user_bias, rest_bias])\n",
    "\n",
    "# reducing the first layer further to 10 node\n",
    "rating = layers.Dense(10, activation='relu')(rating)\n",
    "# increasing the dropout rate to 0.2\n",
    "rating = layers.Dropout(0.6)(rating)\n",
    "# output layer\n",
    "rating = layers.Dense(1, activation='sigmoid')(rating)\n",
    "# conertion of output rating\n",
    "rating = layers.Lambda(lambda x:x*(max_rating - min_rating) + min_rating)(rating)\n",
    "\n",
    "model_2 = models.Model([user, restaurant], rating)\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile( optimizer= 'sgd',\n",
    "                loss='mse', \n",
    "                metrics= [metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Train the model\n",
    "model_2.fit(x=[X_train[:,0], X_train[:,1]], y=y_train,\n",
    "            batch_size=256, \n",
    "            epochs=20, \n",
    "            verbose=1,\n",
    "            validation_data=([X_test[:,0], X_test[:,1]], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The third model has further overfitted the training data as it has high validation score and low training score.\n",
    "Therefore our best neural model is baseline model which has a validation score of 1.3179."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "\u001b[1m13380/13380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 27ms/step - loss: 1.1958 - root_mean_squared_error: 1.0885\n",
      "[1.1989389657974243, 1.089964509010315]\n",
      "Testing data: \n",
      "\u001b[1m3345/3345\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 27ms/step - loss: 1.7751 - root_mean_squared_error: 1.3282\n",
      "[1.779281497001648, 1.3298012018203735]\n"
     ]
    }
   ],
   "source": [
    "# evaluating the best model on the training data\n",
    "print(\"Training data: \")\n",
    "print(baseline_model.evaluate([X_train[:,0], X_train[:,1]], y_train))\n",
    "\n",
    "# evaluating the best model on the test data\n",
    "print(\"Testing data: \")\n",
    "print(baseline_model.evaluate([X_test[:,0], X_test[:,1]], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The baseline model has a training RMSE of 1.1635 and a test RMSE of 1.302 hence being our better neural networks model with the lowest test scores.\n",
    "\n",
    "In all the models SVD has emerged to be the best RMSE score of 1.25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
