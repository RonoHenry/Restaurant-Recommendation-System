{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINDINGS AND EVALUATION BASED ON EDA\n",
    "\n",
    "1) Most common ratings are 4.0 and 3.5 stars, indicating general satisfaction.Ratings below 2.5 stars are uncommon, showing few very poor experiences.\n",
    "\n",
    "\n",
    "2) Pennsylvania (PA), Florida (FL) and Tennessee (TN) boast the highest numbers of restaurants among the states, highlighting them as major markets for the restaurant industry.\n",
    "\n",
    "\n",
    "3) 66.9% Open Restaurants: A significant majority of the restaurants in our dataset are currently operational. This proportion suggests a robust industry where the majority of listed entities are active, presenting numerous options for recommendations.\n",
    "\n",
    "\n",
    "4) 33.1% Closed Restaurants: A substantial portion of the restaurants are no longer in operation. This data is crucial for ensuring accuracy in our recommendation system by filtering out closed establishments, thereby improving user experience and system reliability.\n",
    "\n",
    "\n",
    "5) McDonald's, Subway, and Taco Bell lead in frequency, indicating a high presence of these chains within the dataset. This suggests a dominance of fast-food chains in the restaurant landscape.\n",
    "\n",
    "\n",
    "6) The highest review counts appear at 4.0 and 5.0 star ratings, suggesting high customer engagement with top-rated restaurants and the opposite is true.\n",
    "\n",
    "\n",
    "7) Seasonal Trends: Frequent peaks between April and July, indicating increased customer engagement during this period.\n",
    "\n",
    "\n",
    "8) Decline in Engagement: Noticeable drops in reviews occur after July, particularly in September and December."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL FINDINGS AND EVALUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This section provides a detailed comparative analysis of the models based on their performance metrics. The primary goal is to develop and deploy a robust restaurant recommender system to help customers find the best dining experience.\n",
    "\n",
    "\n",
    "In this project, we will concentrate on three specific types of recommendation models:\n",
    "\n",
    "- Content-Based Recommender Systems\n",
    "\n",
    "\n",
    "- Collaborative Filtering Systems\n",
    "\n",
    "\n",
    "- Deep Neural Networks\n",
    "\n",
    "\n",
    "Within each category, we will evaluate and compare different models to determine which performs the best. For validation and comparison, we will use the RMSE (root mean squared error) metric to measure how closely the predictions align with the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLABORATIVE  BASED MODELS\n",
    "\n",
    "#### 1) BASELINE MODEL  - NORMAL PREDICTOR\n",
    "\n",
    "We initialized a NormalPredictor model, trained it on a training dataset, and then used it to predict ratings for a test dataset. Finally, it computed the root mean square error (RMSE) to evaluate the prediction accuracy.\n",
    "\n",
    "RMSE value of 0.8192 shows a poor model perfomance,so we try model number 2\n",
    "\n",
    "\n",
    "#### MODEL 2 - NMF(Non-Negative Matrix Factorization) MODEL\n",
    "\n",
    "The NMF model had an RMSE of 0.3489 which shows improved model perfomance.\n",
    "\n",
    "\n",
    "#### MODEL 3 - SVD (Single Value Decomposition)\n",
    "\n",
    "**Vanilla model**\n",
    "\n",
    "The findings for the vanilla SVD include:\n",
    "\n",
    "\n",
    "- **Average Model Accuracy**: The mean RMSE of 0.1171 suggests that, on average, the model's predictions are approximately 0.1171 units away from the true values. While this provides a baseline for model accuracy, further tuning or alternative models might be explored to reduce this error.\n",
    "\n",
    "\n",
    "To adress these shortcomings,we carry out  hyperparameter tuning for the SVD model using GridSearchCV. It defines a parameter grid with different values for latent factors and regularization terms. GridSearchCV is then used to evaluate these parameters across the dataset to find the best combination. The model is fitted with the dataset to determine the optimal hyperparameters that yield the best performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuned SVD model 1**\n",
    "\n",
    "The observations for the tuned model include:\n",
    "\n",
    "- *Minimal Improvement*: The best RMSE after hyperparameter tuning (0.11368471505593795) is very close to the initial mean RMSE of 0.1171, indicating that the tuning provided only a slight improvement. This suggests that the model's performance might be close to its maximum potential for this specific dataset and approach.\n",
    "\n",
    "**Tuned SVD model 2**\n",
    "\n",
    "The observations include:\n",
    "\n",
    "- *Improvement in RMSE*: There is a significant improvement of RMSE from 0.11368471505593795 to 0.0683\n",
    "\n",
    "\n",
    "- *Higher Complexity*: The optimal number of factors (n_factors = 100) is on the higher end of the tested range, which could imply that the model benefits from increased complexity, capturing more intricate patterns in the data.\n",
    "\n",
    "\n",
    "- *Low Regularization*: The best regularization term (reg_all = 0.02) is relatively low, indicating that the model performs better with less penalty on the size of the coefficients. This may suggest that the model is less prone to overfitting on this dataset.\n",
    "\n",
    "In conclusion,The RMSE value for the optimized SVD model which is the best performing model so far, is approximately 0.0683, indicating the model's average prediction error in terms of user ratings. Lower RMSE values are desirable as they signify better predictive accuracy.                              \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Deep Neural Networks (DNN)\n",
    "\n",
    "We will employ a Keras deep neural network to develop a recommendation system, aiming to enhance our RMSE scores through the use of neural network techniques.\n",
    "\n",
    "**Vanilla model**\n",
    "\n",
    "The Keras code defines a deep neural network for a recommendation system with a three-layer architecture: the first dense layer has 30 nodes with ReLU activation, the second dense layer has 15 nodes with ReLU activation, and the output layer has one node with a sigmoid activation to produce ratings between 0 and 1. A Lambda layer scales these predictions to a range of 1 to 5. The model is compiled using SGD optimizer, MSE loss function, and RMSE as a metric. It is trained on X_train and y_train, with validation on X_test and y_test, for 10 epochs with a batch size of 256.\n",
    "\n",
    "Here’s a summary of the model training results over 10 epochs:\n",
    "\n",
    "- Epoch 1: Initial RMSE is 1.5015 with a validation RMSE of 1.5007. The training loss is 2.2653.\n",
    "\n",
    "- Epoch 2-4: RMSE improves slightly to 1.4983 in epoch 2, and remains stable around 1.4998 to 1.5011 in subsequent epochs. The validation RMSE also stabilizes around 1.5002 to 1.5005.\n",
    "\n",
    "- Epoch 5-7: Further improvement is observed with RMSE decreasing to 1.4994 by epoch 5 and to 1.4993 by epoch 7. The validation RMSE decreases to 1.4980.\n",
    "\n",
    "- Epoch 8-10: Significant improvement is seen with RMSE dropping to 1.4950 by epoch 8, and further decreasing to 1.4780 by epoch 9. By epoch 10, RMSE reaches 1.4409, with the validation RMSE at 1.4512.\n",
    "\n",
    "In conclusion, the model shows a gradual improvement in RMSE and validation RMSE throughout the epochs, with the most significant gains occurring in the later epochs.\n",
    "\n",
    "\n",
    "**Tuned model 1**\n",
    "\n",
    "The model first concatenates user and restaurant embeddings, then adds bias terms before passing the result through a dense layer with 15 neurons and L2 regularization to help prevent overfitting. A dropout layer with a rate of 0.3 is applied to further mitigate overfitting. The output layer uses a sigmoid activation to generate ratings between 0 and 1, which are then scaled to the range between min_rating and max_rating. The model is compiled with the SGD optimizer and MSE loss, evaluating performance using RMSE. It is trained for 20 epochs with a batch size of 256, and validation is conducted on X_test and y_test.\n",
    "\n",
    "Here’s a summary of the training results for the deep neural network model over 20 epochs:\n",
    "\n",
    "- Epoch 1: The initial training RMSE is 1.4413, and validation RMSE is 1.4232, indicating the model is starting to learn.\n",
    "\n",
    "- Epoch 5: Training RMSE improves to 1.1802, while validation RMSE is 1.3785, showing progress in reducing error.\n",
    "\n",
    "- Epoch 10: The model's RMSE is 1.0074 for training and 1.3851 for validation, indicating a reduction in training error but some fluctuation in validation performance.\n",
    "\n",
    "- Epoch 15: Training RMSE reaches 0.9267, and validation RMSE is 1.3883, suggesting the model is learning well but validation error shows some instability.\n",
    "\n",
    "- Epoch 20: Final training RMSE is 0.8703, with a validation RMSE of 1.3940, reflecting a good fit on training data but slight overfitting or instability on validation data.\n",
    "\n",
    "\n",
    "**Tuned model 2**\n",
    "\n",
    "The model concatenates user and restaurant embeddings and adds bias terms before passing the result through a dense layer with 10 neurons and ReLU activation. A dropout layer with a 0.6 rate is used to prevent overfitting. The output layer, with a sigmoid activation, produces ratings between 0 and 1, which are then scaled to the desired rating range. The model is compiled using the SGD optimizer and MSE loss, and it is trained for 20 epochs with a batch size of 256, validated on X_test and y_test.\n",
    "\n",
    "Here’s a summary of the training results for the new model:\n",
    "\n",
    "- Initial Performance: At the beginning of training, the model had a root mean squared error (RMSE) of 1.2586 on the training set and 1.3816 on the validation set.\n",
    "\n",
    "- Improvement Trend: RMSE steadily improved over epochs, reaching 0.9310 on the training set by epoch 20, but the validation RMSE remained relatively high at 1.4233.\n",
    "\n",
    "- Validation Performance: The validation loss and RMSE showed less improvement compared to the training set, indicating potential overfitting or issues with generalization.\n",
    "\n",
    "- Dropout Effect: Despite increasing the dropout rate to 0.6, which typically helps with regularization, the model did not significantly outperform earlier versions, suggesting the need for additional tuning or adjustments.\n",
    "\n",
    "- Epoch Duration: Each epoch took around 220 to 270 seconds to complete, with a slight increase in training time towards the end, likely due to the model complexity and size of the dataset.\n",
    "\n",
    "In conclusion,the third model has further overfitted the training data as it has high validation score and low training score.\n",
    "Therefore our best neural model is baseline model which has a validation score of 1.3179.This is however still poor compared to the optimized SVD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the SVD and the best peforming DNN\n",
    "\n",
    "1. Error Metrics\n",
    "\n",
    "The Singular Value Decomposition (SVD) model achieves a significantly lower RMSE of 0.0683, indicating superior accuracy in predicting ratings. In contrast, the Deep Neural Network (DNN) model has a much higher RMSE of 1.4409, suggesting that its predictions deviate more from the actual ratings. This substantial difference highlights SVD’s better performance in minimizing prediction error.\n",
    "\n",
    "\n",
    "\n",
    "2. Model Complexity\n",
    "\n",
    "SVD is relatively simple and requires fewer computational resources, as it involves basic matrix decomposition with fewer parameters and epochs. On the other hand, DNN is complex, requiring extensive training with multiple layers and hyperparameters, which increases computational cost and complexity. This makes SVD a more efficient choice in terms of model complexity and resource usage.\n",
    "\n",
    "\n",
    "\n",
    "3. Training and Validation Time\n",
    "\n",
    "Training the DNN model is time-consuming, with each epoch taking between 246 to 334 seconds, and requiring 10 epochs for completion. In contrast, SVD typically involves fewer epochs and is computationally less intensive, leading to faster training times. This efficiency in SVD makes it more practical for scenarios where quick model iteration is needed.\n",
    "\n",
    "\n",
    "\n",
    "4. Overfitting and Generalization\n",
    "\n",
    "The DNN model shows a higher RMSE on validation data compared to training data, which may indicate overfitting and reduced generalization. Conversely, the SVD model exhibits low and consistent RMSE values across both training and validation sets, suggesting better generalization and a lower risk of overfitting. This makes SVD a more reliable model for unseen data.\n",
    "\n",
    "\n",
    "\n",
    "5. Interpretability and Usability\n",
    "SVD is more interpretable, as it breaks down the user-item matrix into latent factors, making it easier to understand how recommendations are generated. The DNN model, with its complex architecture, functions more like a \"black box,\" making it harder to decipher the underlying decision-making process. This interpretability advantage of SVD provides clearer insights into the recommendation mechanics.\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Despite improvements, the DNN’s final RMSE is higher than the SVD model's, suggesting that the SVD model performs better with this dataset and may require less tuning.\n",
    "\n",
    "We will deploy the optimized SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECOMMENDATIONS\n",
    "\n",
    "1) *Finding: Most common ratings are 4.0 and 3.5 stars, indicating general satisfaction.*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Filter Out Lower Ratings**\n",
    "\n",
    "Although very poor ratings are rare, consider implementing a filter or threshold that minimizes the visibility of restaurants with ratings below a certain level (e.g., 3.0 stars). This will help ensure users are less likely to encounter options with significant negative feedback.\n",
    "\n",
    "- **Include User Preferences**\n",
    "\n",
    "Allow users to set preferences for certain rating ranges, so they can prioritize restaurants that align with their expectations. For example, a user might prefer options that are rated above 3.5 stars.\n",
    "\n",
    "- **Feature High-Rated Restaurants Prominently**\n",
    " \n",
    "Highlight in Recommendations: Ensure that top-rated restaurants are prominently featured in recommendations, especially for users seeking highly-rated options.\n",
    "\n",
    "\n",
    "2) *Finding: Pennsylvania (PA), Florida (FL) and Tennessee (TN) boast the highest numbers of restaurants among the states*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "\n",
    "- **Prioritize Major Markets for Data**\n",
    "\n",
    "Focus on PA, FL, and TN: Since these states have the highest number of restaurants, ensure that your dataset is comprehensive and up-to-date for these regions. Consider implementing advanced filtering options and more detailed reviews for users in these states, as there’s likely to be higher demand and a wider variety of choices.\n",
    "\n",
    "\n",
    "- **Tailor Recommendations to Market Size**\n",
    "\n",
    "Adjust the depth of recommendations based on the density of restaurants in the region. For major markets (like PA, FL, TN), provide more granular and diverse recommendations. For regions with fewer options (like NC, CO, HI, MT), focus on quality over quantity and highlight the best available choices.\n",
    "\n",
    "\n",
    "- **Offer Diverse Data Insights**\n",
    "\n",
    "Highlight Popular and Hidden Gems: In high-density areas, showcase a mix of popular spots and lesser-known but highly rated restaurants. In less populated states, emphasize unique and standout options to ensure users still have great dining experiences despite fewer choices.\n",
    "\n",
    "3) *Finding: 66.9% are Open Restaurants and 33.1% are Closed Restaurants:*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "\n",
    "- **Prioritize Accuracy by Filtering Closed Restaurants**\n",
    "\n",
    "Implement Robust Filters: Ensure that your system rigorously filters out closed restaurants. Regularly update your dataset to reflect the current operational status of restaurants, maintaining accuracy in your recommendations.\n",
    "\n",
    "\n",
    "- **Automated Alerts**\n",
    "\n",
    "Set up automated systems to flag and remove listings of closed restaurants as soon as possible.\n",
    "\n",
    "4) *Finding: McDonald's, Subway,  Taco Bell,Wendy's, Domino's Pizza, and Pizza Hut lead in frequency. This suggests a dominance of fast-food chains in the restaurant landscape.*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Leverage Popularity for Recommendations**\n",
    "\n",
    "Feature Popular Chains Prominently: Highlight McDonald's, Subway, Taco Bell, Wendy's, Domino's Pizza, and Pizza Hut in your recommendations. Users often seek familiar and reliable options, so showcasing these popular chains can enhance user satisfaction.\n",
    "\n",
    "\n",
    "- **Market and User Segmentation**\n",
    "\n",
    "Targeted Recommendations: Use segmentation to tailor recommendations based on user demographics or previous behavior. For example, users who frequently choose fast-food chains might see more recommendations for similar options.\n",
    "\n",
    "\n",
    "5) *Finding: Seasonal Trends: Frequent peaks between April and July, indicating increased customer engagement during this period.*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Incorporate Seasonality into Recommendations** \n",
    "\n",
    "Highlight Seasonal Popularity: During peak periods (April to July), increase the visibility of restaurants that typically see higher engagement. You might feature seasonal promotions or highlight trending restaurants during these months.\n",
    "\n",
    "6)  *Finding: Not all popular cuisines are included in the list of options*\n",
    "\n",
    "The recommendations based on this finding are:\n",
    "\n",
    "- **Cuisine diversification**\n",
    "\n",
    "Feature variations of cuisines from different regions (e.g., Southern Italian vs. Northern Italian) to give users a more nuanced dining experience.\n",
    "\n",
    "- **Dietary Preferences**\n",
    "\n",
    "Include filters for dietary preferences and restrictions (e.g., vegetarian, vegan, gluten-free) to provide more personalized recommendations.\n",
    "\n",
    "- **Cuisine Exploration**\n",
    "\n",
    "Offer recommendations based on cuisine trends and new dining experiences. For example, feature emerging cuisines or new restaurant concepts that might interest adventurous diners.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
